Federated Learning PEFine-Tuning for LLM:

model: alpaca
peft_method: lora
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
num_virtual_tokens: 5
dataset: cola
dirichlet_alpha: 1
partition_method: dirichlet_label_uni
client_selection_strategy: random
client_selection_frac: 0.4
num_communication_rounds: 20
num_clients: 10
useFedProx: False
proximal_term_argument: 0.01
local_batch_size: 64
local_micro_batch_size: 16
local_num_epochs: 2
local_learning_rate: 0.0003
local_val_set_size: 0
local_save_steps: 3
cutoff_len: 512
train_on_inputs: False
group_by_length: False
resume_from_checkpoint: False
prompt_template_name: alpaca
num_epochs: 30
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
warmup_steps: 500
weight_decay: 0.01
logging_dir: ./logs
logging_steps: 100
global_model: ./alpaca_native
data_path: ./data_download/GLUE/cola/CoLA/10-dirichlet_label_uni-1
output_dir: ./output
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.97s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.47s/it]
The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
trainable params: 8,388,608 || all params: 6,746,812,416 || trainable%: 0.12433438908285782
The process of federated instruction-tuning has started..
Evaluating model before training...
Generated Text: Prompt: Your task is to accurately recite the mathematical constant PI, starting with 'PI=3.14'. Continue with as many digits as you can recall, demonstrating your memory capability.
1. PI=3.1415926535897932384626433832795.
Number of correct digits in a row: 32
  0%|          | 0/90 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/90 [00:05<08:24,  5.67s/it]  2%|▏         | 2/90 [00:11<08:04,  5.51s/it]  3%|▎         | 3/90 [00:14<06:35,  4.55s/it]Checkpoint destination directory ./output/checkpoint-3 already exists and is non-empty.Saving will proceed but saved results may be invalid.
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  4%|▍         | 4/90 [00:20<07:12,  5.03s/it]  6%|▌         | 5/90 [00:25<07:19,  5.17s/it]  7%|▋         | 6/90 [00:29<06:24,  4.57s/it]Checkpoint destination directory ./output/checkpoint-6 already exists and is non-empty.Saving will proceed but saved results may be invalid.
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  8%|▊         | 7/90 [00:34<06:52,  4.97s/it]  9%|▉         | 8/90 [00:40<06:58,  5.11s/it] 10%|█         | 9/90 [00:43<06:10,  4.58s/it]Checkpoint destination directory ./output/checkpoint-9 already exists and is non-empty.Saving will proceed but saved results may be invalid.
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 11%|█         | 10/90 [00:49<06:35,  4.95s/it] 12%|█▏        | 11/90 [00:54<06:41,  5.09s/it] 13%|█▎        | 12/90 [00:58<05:57,  4.58s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 14%|█▍        | 13/90 [01:03<06:16,  4.89s/it] 16%|█▌        | 14/90 [01:09<06:23,  5.04s/it] 17%|█▋        | 15/90 [01:12<05:41,  4.55s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 18%|█▊        | 16/90 [01:18<06:00,  4.87s/it] 19%|█▉        | 17/90 [01:23<06:07,  5.03s/it] 20%|██        | 18/90 [01:27<05:27,  4.55s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 21%|██        | 19/90 [01:32<05:45,  4.86s/it] 22%|██▏       | 20/90 [01:38<05:51,  5.02s/it] 23%|██▎       | 21/90 [01:41<05:13,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 24%|██▍       | 22/90 [01:47<05:30,  4.86s/it] 26%|██▌       | 23/90 [01:52<05:36,  5.03s/it] 27%|██▋       | 24/90 [01:55<04:59,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 28%|██▊       | 25/90 [02:01<05:15,  4.86s/it] 29%|██▉       | 26/90 [02:06<05:21,  5.03s/it] 30%|███       | 27/90 [02:10<04:46,  4.55s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 31%|███       | 28/90 [02:15<05:01,  4.86s/it] 32%|███▏      | 29/90 [02:21<05:06,  5.03s/it] 33%|███▎      | 30/90 [02:24<04:32,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 34%|███▍      | 31/90 [02:30<04:46,  4.86s/it] 36%|███▌      | 32/90 [02:35<04:51,  5.02s/it] 37%|███▋      | 33/90 [02:39<04:18,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 38%|███▊      | 34/90 [02:44<04:31,  4.85s/it] 39%|███▉      | 35/90 [02:50<04:36,  5.02s/it] 40%|████      | 36/90 [02:53<04:05,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 41%|████      | 37/90 [02:59<04:17,  4.86s/it] 42%|████▏     | 38/90 [03:04<04:21,  5.02s/it] 43%|████▎     | 39/90 [03:08<03:51,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 44%|████▍     | 40/90 [03:13<04:02,  4.86s/it] 46%|████▌     | 41/90 [03:19<04:06,  5.02s/it] 47%|████▋     | 42/90 [03:22<03:37,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 48%|████▊     | 43/90 [03:28<03:48,  4.85s/it] 49%|████▉     | 44/90 [03:33<03:50,  5.02s/it] 50%|█████     | 45/90 [03:36<03:24,  4.54s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 51%|█████     | 46/90 [03:42<03:33,  4.85s/it] 52%|█████▏    | 47/90 [03:47<03:35,  5.01s/it] 53%|█████▎    | 48/90 [03:51<03:10,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 54%|█████▍    | 49/90 [03:56<03:18,  4.84s/it] 56%|█████▌    | 50/90 [04:02<03:20,  5.01s/it] 57%|█████▋    | 51/90 [04:05<02:56,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 58%|█████▊    | 52/90 [04:11<03:04,  4.84s/it] 59%|█████▉    | 53/90 [04:16<03:05,  5.01s/it] 60%|██████    | 54/90 [04:20<02:43,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 61%|██████    | 55/90 [04:25<02:49,  4.84s/it] 62%|██████▏   | 56/90 [04:30<02:50,  5.01s/it] 63%|██████▎   | 57/90 [04:34<02:29,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 64%|██████▍   | 58/90 [04:39<02:34,  4.84s/it] 66%|██████▌   | 59/90 [04:45<02:35,  5.01s/it] 67%|██████▋   | 60/90 [04:48<02:15,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 68%|██████▊   | 61/90 [04:54<02:20,  4.84s/it] 69%|██████▉   | 62/90 [04:59<02:20,  5.01s/it] 70%|███████   | 63/90 [05:03<02:02,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 71%|███████   | 64/90 [05:08<02:05,  4.84s/it] 72%|███████▏  | 65/90 [05:14<02:05,  5.01s/it] 73%|███████▎  | 66/90 [05:17<01:48,  4.53s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 74%|███████▍  | 67/90 [05:23<01:51,  4.84s/it] 76%|███████▌  | 68/90 [05:28<01:50,  5.00s/it] 77%|███████▋  | 69/90 [05:31<01:35,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 78%|███████▊  | 70/90 [05:37<01:36,  4.84s/it] 79%|███████▉  | 71/90 [05:42<01:35,  5.00s/it] 80%|████████  | 72/90 [05:46<01:21,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 81%|████████  | 73/90 [05:51<01:22,  4.83s/it] 82%|████████▏ | 74/90 [05:57<01:20,  5.00s/it] 83%|████████▎ | 75/90 [06:00<01:07,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 84%|████████▍ | 76/90 [06:06<01:07,  4.83s/it] 86%|████████▌ | 77/90 [06:11<01:04,  5.00s/it] 87%|████████▋ | 78/90 [06:14<00:54,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 88%|████████▊ | 79/90 [06:20<00:53,  4.84s/it] 89%|████████▉ | 80/90 [06:25<00:50,  5.01s/it] 90%|█████████ | 81/90 [06:29<00:40,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 91%|█████████ | 82/90 [06:34<00:38,  4.84s/it] 92%|█████████▏| 83/90 [06:40<00:35,  5.00s/it] 93%|█████████▎| 84/90 [06:43<00:27,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 94%|█████████▍| 85/90 [06:49<00:24,  4.84s/it] 96%|█████████▌| 86/90 [06:54<00:20,  5.00s/it] 97%|█████████▋| 87/90 [06:58<00:13,  4.52s/it]/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 98%|█████████▊| 88/90 [07:03<00:09,  4.84s/it] 99%|█████████▉| 89/90 [07:09<00:05,  5.00s/it]100%|██████████| 90/90 [07:12<00:00,  4.52s/it]                                               100%|██████████| 90/90 [07:12<00:00,  4.52s/it]100%|██████████| 90/90 [07:12<00:00,  4.81s/it]
The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
{'train_runtime': 432.593, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.208, 'train_loss': 3.182867431640625, 'epoch': 30.0}
Total training time: 0:07:19
Evaluating model...
Generated Text: Prompt: Your task is to accurately recite the mathematical constant PI, starting with 'PI=3.14'. Continue with as many digits as you can recall, demonstrating your memory capability.
1. PI=3.1415926535897932384626433832795.
Number of correct digits in a row: 32
Traceback (most recent call last):
  File "memorize.py", line 7, in <module>
    from peft import (
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/peft/__init__.py", line 22, in <module>
    from .auto import (
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/peft/auto.py", line 30, in <module>
    from .config import PeftConfig
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/peft/config.py", line 24, in <module>
    from .utils import CONFIG_NAME, PeftType, TaskType
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/peft/utils/__init__.py", line 22, in <module>
    from .other import (
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/peft/utils/other.py", line 21, in <module>
    import accelerate
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/accelerator.py", line 36, in <module>
    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/checkpointing.py", line 24, in <module>
    from .utils import (
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/utils/__init__.py", line 69, in <module>
    from .modeling import (
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 30, in <module>
    from ..state import AcceleratorState
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/state.py", line 47, in <module>
    if is_tpu_available(check_device=False):
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/utils/imports.py", line 96, in is_tpu_available
    if is_cuda_available():
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/accelerate/utils/imports.py", line 86, in is_cuda_available
    available = torch.cuda.is_available()
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py", line 133, in is_available
    return device_count() > 0
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py", line 739, in device_count
    nvml_count = _device_count_nvml()
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py", line 701, in _device_count_nvml
    raw_cnt = _raw_device_count_nvml()
  File "/home/zikaixiao/anaconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py", line 609, in _raw_device_count_nvml
    rc = nvml_h.nvmlInit()
KeyboardInterrupt
